{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fulea-Andrei/ProjectInfoVisualizationCourse/blob/main/info_visualization_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BP04LJ7rsgM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from numpy import mean\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
        "import math\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from nltk import everygrams\n",
        "from nltk import FreqDist\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "from nltk.stem.snowball import RomanianStemmer\n",
        "import unidecode\n",
        "from nltk import sent_tokenize\n",
        "from sklearn.linear_model import Lasso\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import Ridge\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import seaborn as sbn\n",
        "import pandas as pd\n",
        "import re\n",
        "import stop_words\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem.snowball import RomanianStemmer\n",
        "import unidecode\n",
        "from nltk.util import ngrams\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import missingno as msno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh1fiFUDvXNx"
      },
      "outputs": [],
      "source": [
        "completed_websites_dataset = pd.read_csv(\"https://raw.githubusercontent.com/Fulea-Andrei/ProjectInfoVisualizationCourse/main/national_real_estate_data.csv\",index_col=0,low_memory=False)\n",
        "national_real_estate_data = completed_websites_dataset.copy()\n",
        "ds_cleaned = pd.read_csv(\"https://raw.githubusercontent.com/Fulea-Andrei/ProjectInfoVisualizationCourse/main/national_real_estate_data_CLEANED.csv\",index_col=0)\n",
        "world_cities = pd.read_csv(\"https://raw.githubusercontent.com/Fulea-Andrei/ProjectInfoVisualizationCourse/main/worldcities.csv\",index_col=0)\n",
        "ds_labeled = pd.read_csv(\"https://raw.githubusercontent.com/Fulea-Andrei/DissertationProject/main/date/dataset_labeled.csv?token=GHSAT0AAAAAABUEP42VC5WH72LXOKI24J3WYUGCSEQ\",index_col=0)\n",
        "print(national_real_estate_data.columns)\n",
        "city_ids = {\n",
        "    'maramures': 1642377306,\n",
        "    'iasi' : 1642367695,\n",
        "    'timis' : 1642603121,\n",
        "    'timisoara' : 1642603121,\n",
        "    'brasov' : 1642328429,\n",
        "    'bucuresti' : 1642414442,\n",
        "    'sibiu' : 1642393086,\n",
        "    'cluj' : 1642503974,\n",
        "    'constanta' : 1642851858,\n",
        "    'bucuresti-ilfov' :  1642918141,\n",
        "    'oradea' : 1642066626\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7jBU6jYMF19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4b22f9-76b3-40b7-96ca-810bc14accae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45015/45015 [00:00<00:00, 900356.67it/s]\n"
          ]
        }
      ],
      "source": [
        "class CleaningDataSet():\n",
        "  def __init__(self, data_frame):\n",
        "    self.data_frame = data_frame\n",
        "\n",
        "  def bathrooms_(self):\n",
        "    bathrooms = list(self.data_frame['bathrooms'].values)\n",
        "    bathrooms_list_cleaned_int = [int(x) for x in bathrooms] \n",
        "    return bathrooms_list_cleaned_int\n",
        "\n",
        "\n",
        "  def town_mapping(self):\n",
        "    cities_string = list(self.data_frame['town'].values)\n",
        "    cities_numbers = []\n",
        "    for city in tqdm(cities_string):\n",
        "       cities_numbers.append(city_ids[city])\n",
        "    return cities_numbers\n",
        "      \n",
        "\n",
        "  def get_cosine(self,vec1, vec2):\n",
        "      intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "      numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "      sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
        "      sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
        "      denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "      if not denominator:\n",
        "          return 0.0\n",
        "      else:\n",
        "          return float(numerator) / denominator\n",
        "\n",
        "\n",
        "  def text_to_vector(self,text):\n",
        "      try:\n",
        "        words = self.WORD.findall(text)\n",
        "      except Exception as err:\n",
        "        print(err)\n",
        "      else:\n",
        "        return Counter(words)\n",
        "                       \n",
        "\n",
        "  def labeling_data_set(self):\n",
        "\n",
        "    tags_luxury_assets = ['spatioasa','mobilata','nou','investitie','bloc nou','centru','utilata','modern','calitate']\n",
        "    tags_nonluxury_assets = ['margine','nemobilat']\n",
        "\n",
        "    \n",
        "    descriptions = list(self.data_frame['description'].values)\n",
        "    contor = 0\n",
        "    dataset_labeled = self.data_frame.copy()\n",
        "    dataset_labeled['label'] = [''] * len(dataset_labeled)\n",
        "\n",
        "    for desc in tqdm(descriptions):\n",
        "      if pd.isna(desc) == False:\n",
        "          for tag in tags_luxury_assets:\n",
        "            clf = CleaningDataSet(self.data_frame)\n",
        "            tag_ = clf.text_to_vector(tag,0)\n",
        "            desc_ = clf.text_to_vector(desc,1)\n",
        "            cosine = clf.get_cosine(tag_, desc_)\n",
        "            if cosine > 0.0:\n",
        "              contor += 1\n",
        "          \n",
        "          num_feature = dataset_labeled.loc[dataset_labeled['description'] == desc, 'price'].iloc[0]\n",
        "   \n",
        "          if (contor >= len(tags_luxury_assets)/2) and (len(word_tokenize(desc)) > 150): #and (num_feature > 80000):\n",
        "            dataset_labeled.loc[dataset_labeled['description'] == desc, 'label'] = \"luxury_asset\"\n",
        "      elif pd.isna(desc) == True:\n",
        "            dataset_labeled.loc[dataset_labeled['description'] == desc, 'label'] = \"nan\"\n",
        "\n",
        "    ##########################################################################\n",
        "    for desc in tqdm(descriptions):\n",
        "        if pd.isna(desc) == False:\n",
        "            for tag in tags_nonluxury_assets:\n",
        "              clf = CleaningDataSet(self.data_frame)\n",
        "              tag_ = clf.text_to_vector(tag,0)\n",
        "              desc_ = clf.text_to_vector(desc,1)\n",
        "              cosine = clf.get_cosine(tag_, desc_)\n",
        "              if cosine > 0.0:\n",
        "                contor += 1\n",
        "\n",
        "            if (contor >= len(tags_nonluxury_assets)/2) and (len(word_tokenize(desc)) <= 150): #and (num_feature <= 80000):\n",
        "              dataset_labeled.loc[dataset_labeled['description'] == desc, 'label'] = \"modest_asset\"\n",
        "        elif pd.isna(desc) == True:\n",
        "              dataset_labeled.loc[dataset_labeled['description'] == desc, 'label'] = \"nan\"\n",
        "            \n",
        "\n",
        "\n",
        "    return dataset_labeled\n",
        "\n",
        "\n",
        "  def assetstate_map(self):\n",
        "\n",
        "    assetstate_map = []\n",
        "    assetstate_list = []\n",
        "    assetstate_list_map = []\n",
        "    assetstate_map = list(set(self.data_frame['assetstate'].values))\n",
        "    assetstate_list = list(self.data_frame['assetstate'].values)\n",
        "    for i in assetstate_list:\n",
        "        if i in assetstate_map:\n",
        "            assetstate_list_map.append(assetstate_map.index(i))\n",
        "            \n",
        "    return assetstate_list_map\n",
        "\n",
        "\n",
        "\n",
        "  def colector_map(self):\n",
        "\n",
        "    colector_map = []\n",
        "    colector_list = []\n",
        "    colector_list_map = []\n",
        "    colector_map = list(set(self.data_frame['colector'].values))\n",
        "    colector_list = list(self.data_frame['colector'].values)\n",
        "    for i in colector_list:\n",
        "        if i in colector_map:\n",
        "            colector_list_map.append(colector_map.index(i))\n",
        "            \n",
        "    return colector_list_map\n",
        "\n",
        "  def compartimentation_map(self):\n",
        "\n",
        "    aux_wrong_list = ['semidecomandat','decomandat','circular','vagon','Nedecomandat','semi-decomandat','comandat','decomandat; det_comp1','nedecomandat']\n",
        "    aux_right_list = ['Semidecomandat','Decomandat','Circular','Vagon','Comandat','Semidecomandat','Comandat','Decomandat','Comandat']\n",
        "    compartimentation_map = []\n",
        "    compartimentation_list = []\n",
        "    compartimentation_list_map = []\n",
        "    compartimentation_list = list(self.data_frame['compartimentation'].values)\n",
        "    \n",
        "    for i,elem in enumerate(compartimentation_list):\n",
        "        if elem in aux_wrong_list:\n",
        "            compartimentation_list[i] = aux_right_list[aux_wrong_list.index(elem)] \n",
        "        \n",
        "    \n",
        "    compartimentation_map = list(set(compartimentation_list))\n",
        "    for i in compartimentation_list:\n",
        "        if i in compartimentation_map:\n",
        "            compartimentation_list_map.append(compartimentation_map.index(i))\n",
        "            \n",
        "    return compartimentation_list_map\n",
        "\n",
        "  def confort_map(self):\n",
        "\n",
        "    \n",
        "    confort_map = []\n",
        "    confort_list = []\n",
        "    confort_list_map = []\n",
        "    confort_list = list(self.data_frame['confort'].values)\n",
        "    \n",
        "    for i,elem in enumerate(confort_list):\n",
        "        if elem == 'lux':\n",
        "            confort_list[i] = 'Lux' \n",
        "        \n",
        "    confort_map = list(set(confort_list))\n",
        "    for i in confort_list:\n",
        "        if i in confort_map:\n",
        "            confort_list_map.append(confort_map.index(i))\n",
        "            \n",
        "    return confort_list_map\n",
        "\n",
        "  def furnished_map(self):\n",
        "\n",
        "    \n",
        "    furnished_map = []\n",
        "    furnished_list = []\n",
        "    furnished_list_map = []\n",
        "    furnished_list = list(self.data_frame['furnished'].values)\n",
        "    \n",
        "    for i,elem in enumerate(furnished_list):\n",
        "        if elem == 'mobilat' or elem == 'mobilat; det_mobila6':\n",
        "            furnished_list[i] = 'Mobilat' \n",
        "               \n",
        "    furnished_map = list(set(furnished_list))\n",
        "    for i in furnished_list:\n",
        "        if i in furnished_map:\n",
        "            furnished_list_map.append(furnished_map.index(i))\n",
        "            \n",
        "    return furnished_list_map\n",
        "\n",
        "\n",
        "  def rooms_cleaning(self):\n",
        "\n",
        "    rooms_list_cleaned = []\n",
        "    rooms_list = list(self.data_frame['rooms'].values)\n",
        "    for i,elem in enumerate(rooms_list):\n",
        "        if type(elem) is float:\n",
        "            rooms_list_cleaned.append(\"0\")\n",
        "        elif len(elem) == 1 or len(elem) == 2: \n",
        "            rooms_list_cleaned.append(elem)\n",
        "        elif len(elem) == 3 or len(elem) == 4: \n",
        "            lst_split = elem.split(\".\")\n",
        "            rooms_list_cleaned.append(lst_split[0])\n",
        "        elif len(elem) > 4 and len(elem) < 10:\n",
        "            lst_split = elem.split()\n",
        "            rooms_list_cleaned.append(lst_split[0])\n",
        "        elif len(elem) > 9:\n",
        "            lst_split = elem.split()\n",
        "            rooms_list_cleaned.append(lst_split[-1])\n",
        "            \n",
        "    rooms_list_cleaned_int = [int(x) for x in rooms_list_cleaned]         \n",
        "    return rooms_list_cleaned_int \n",
        "\n",
        "  def balcony_cleaning(self):\n",
        "    balcony_list_cleaned = []\n",
        "    balcony_list = list(self.data_frame['balcony'].values)\n",
        "    for elem in balcony_list:\n",
        "        if type(elem) is float:\n",
        "            balcony_list_cleaned.append(\"0\")\n",
        "        else:\n",
        "            balcony_list_cleaned.append(elem[0])\n",
        "    \n",
        "    balcony_list_cleaned_int = [int(x) for x in balcony_list_cleaned] \n",
        "    return balcony_list_cleaned_int\n",
        "\n",
        "  def squaremetres_cleaning(self):\n",
        "    squaremetres_list_cleaned = []\n",
        "    squaremetres_list = list(self.data_frame['squaremetres'].values)\n",
        "    for elem in squaremetres_list:\n",
        "        if type(elem) == float:                                        \n",
        "            squaremetres_list_cleaned.append(\"0\")\n",
        "        elif type(elem) == str:\n",
        "            if \",\" in elem and \".\" not in elem and \" \" in elem:   #98,2 mp\n",
        "                elem = elem.replace(\",\", \".\")\n",
        "                lst_split = elem.split()\n",
        "                squaremetres_list_cleaned.append(lst_split[0])\n",
        "            elif \",\" in elem and \".\" not in elem and \" \" not in elem:   #98,2\n",
        "                elem = elem.replace(\",\",\".\")\n",
        "                squaremetres_list_cleaned.append(elem)    \n",
        "            elif \",\" not in elem and \".\" in elem and \" \" in elem:    #98.2 mp      \n",
        "                lst_split = elem.split()\n",
        "                squaremetres_list_cleaned.append(lst_split[0])\n",
        "            elif \",\" not in elem and \".\" in elem and \" \" not in elem:   #98.2\n",
        "                squaremetres_list_cleaned.append(elem)    \n",
        "            elif \",\" not in elem and \".\" not in elem and \" \" in elem:  #98 mp    \n",
        "                lst_split = elem.split()\n",
        "                squaremetres_list_cleaned.append(lst_split[0])    \n",
        "        \n",
        "    squaremetres_list_cleaned_float =  [int(float(x)) for x in squaremetres_list_cleaned] \n",
        "    return squaremetres_list_cleaned_float\n",
        "\n",
        "  def yearconstruction_cleaning(self):\n",
        "    yearconstruction_list_cleaned = []\n",
        "    yearconstruction_list_cleaned_int = []\n",
        "    test = []\n",
        "    yearconstruction_list = list(self.data_frame['yearconstruction'].values)\n",
        "    for elem in yearconstruction_list: \n",
        "        if type(elem) == float:   #nan                                     \n",
        "            yearconstruction_list_cleaned.append(\"0\")\n",
        "        elif type(elem) == str:\n",
        "            if \".\" not in elem and len(elem) == 4:   #1998\n",
        "                yearconstruction_list_cleaned.append(elem)\n",
        "            elif \".\" in elem and \" \" not in elem:   #1998.0\n",
        "                lst_split = elem.split('.')\n",
        "                yearconstruction_list_cleaned.append(lst_split[0])    \n",
        "            elif \"-\" in elem and \" \" not in elem:    #1990-1998      \n",
        "                lst_split = elem.split('-')\n",
        "                yearconstruction_list_cleaned.append(lst_split[1])\n",
        "            elif \"(\" in elem and \")\" in elem and \" \" in elem:   #1998 (in constructie)\n",
        "                lst_split = elem.split()\n",
        "                yearconstruction_list_cleaned.append(lst_split[0])    \n",
        "            elif \"şi\" in elem and \"(\" not in elem and \")\" not in elem:  #intre 1998 si 1998    \n",
        "                lst_split = elem.split()\n",
        "                yearconstruction_list_cleaned.append(lst_split[-1])\n",
        "            elif \"de\" in elem and \"(\" not in elem and \")\" not in elem:  #inainte de 1998    \n",
        "                lst_split = elem.split()\n",
        "                yearconstruction_list_cleaned.append(lst_split[-1]) \n",
        "            else:\n",
        "                test.append(elem)\n",
        "    #print(test)    \n",
        "    yearconstruction_list_cleaned_int =  [int(x) for x in yearconstruction_list_cleaned] \n",
        "    return yearconstruction_list_cleaned_int\n",
        "\n",
        "  def price_cleaning(self):\n",
        "    price_list = []\n",
        "    price_list_cleaned = []\n",
        "    price_list_cleaned_int = []\n",
        "    ramase = []\n",
        "    price_list = list(self.data_frame['price'].values)\n",
        "    \n",
        "    listuta=[]\n",
        "    listuta = self.squaremetres_cleaning()\n",
        "    \n",
        "    for i,elem in enumerate(price_list):\n",
        "        if type(elem) == float:\n",
        "            price_list_cleaned.append(\"0\")\n",
        "        elif type(elem) == str:         \n",
        "            if (\"€\" in elem or \"EUR\" in elem or \"€;\" in elem) and (\"RON\" not in elem and \"€/mp\" not in elem):    \n",
        "                list_split = elem.split()\n",
        "                price_list_cleaned.append(list_split[0].replace(\".\",\"\"))\n",
        "                continue\n",
        "            elif \",\" in elem and \"EUR\" in elem:\n",
        "                price_list_cleaned.append(\"0\")\n",
        "            elif elem == \"bar_chart\":\n",
        "                price_list_cleaned.append(\"0\")\n",
        "                continue\n",
        "            elif elem.replace(\".\", \"\").isnumeric():\n",
        "                price_list_cleaned.append(elem.replace(\".\",\"\"))\n",
        "                continue\n",
        "            elif \"LEI\" in elem:\n",
        "                list_split = elem.split()\n",
        "                price_list_cleaned.append(list_split[0].replace(\".\",\"\"))\n",
        "                continue\n",
        "            elif \"chart\" in elem and \"'\\'\" in elem:\n",
        "                list_split = elem.split()\n",
        "                price_list_cleaned.append(list_split[0].replace(\".\",\"\")) \n",
        "                continue\n",
        "            elif \"RON\" in elem and \"USD\" in elem and \"EUR\" in elem:\n",
        "                aux1 = elem.split(\"D\")\n",
        "                aux2 = aux1[1].split()\n",
        "                price_list_cleaned.append(aux2[0].replace(\".\",\"\"))\n",
        "                continue\n",
        "            elif \"RON\" in elem and \"USD\" in elem and \"EUR\" not in elem: \n",
        "                aux1 = elem.split(\"N\")\n",
        "                aux2 = aux1[1].split()\n",
        "                aux3 = aux2[0].replace(\".\", \"\")\n",
        "                aux3 = int(aux3)\n",
        "                aux3 = aux3 * .9\n",
        "                aux3 = round(aux3)\n",
        "                price_list_cleaned.append(str(aux3).replace(\".\",\"\")) \n",
        "                continue\n",
        "            elif \"ron\" in elem:\n",
        "                price_list_cleaned.append(\"0\")\n",
        "                continue\n",
        "            else:\n",
        "                list_split = elem.split()\n",
        "                metri = str(round(listuta[i]))\n",
        "                pret = int(list_split[0].replace(\".\",\"\")) * int(metri.replace(\".\",\"\"))\n",
        "                price_list_cleaned.append(str(pret).replace(\".\",\"\"))\n",
        "    \n",
        "    price_list_cleaned_int =  [int(\"0\") if \",\" in x else int(x) for x in price_list_cleaned]            \n",
        "    return price_list_cleaned_int\n",
        "\n",
        "\n",
        "datasetCleanerObject = CleaningDataSet(national_real_estate_data)\n",
        "\n",
        "city_ids = datasetCleanerObject.town_mapping()\n",
        "\n",
        "assetStateMap = datasetCleanerObject.assetstate_map()\n",
        "colectorMap = datasetCleanerObject.colector_map()\n",
        "compartimentationMap = datasetCleanerObject.compartimentation_map()\n",
        "confortMap = datasetCleanerObject.confort_map()\n",
        "furnishedMap = datasetCleanerObject.furnished_map()\n",
        "roomsCleaning = datasetCleanerObject.rooms_cleaning()\n",
        "balconyCleaning = datasetCleanerObject.balcony_cleaning()\n",
        "smCleaning = datasetCleanerObject.squaremetres_cleaning()\n",
        "ycCleaning = datasetCleanerObject.yearconstruction_cleaning()\n",
        "priceCleaning = datasetCleanerObject.price_cleaning()\n",
        "# bathrooms_= datasetCleanerObject.bathrooms_()\n",
        "\n",
        "national_real_estate_data_CLEANED = national_real_estate_data.copy()\n",
        "\n",
        "national_real_estate_data_CLEANED['assetstate'] = assetStateMap\n",
        "national_real_estate_data_CLEANED['colector'] = colectorMap\n",
        "national_real_estate_data_CLEANED['compartimentation'] = compartimentationMap\n",
        "national_real_estate_data_CLEANED['confort'] = confortMap\n",
        "national_real_estate_data_CLEANED['furnished'] = furnishedMap\n",
        "national_real_estate_data_CLEANED['rooms'] = roomsCleaning\n",
        "national_real_estate_data_CLEANED['balcony'] = balconyCleaning\n",
        "national_real_estate_data_CLEANED['squaremetres'] = smCleaning\n",
        "national_real_estate_data_CLEANED['yearconstruction'] = ycCleaning\n",
        "national_real_estate_data_CLEANED['price'] = priceCleaning\n",
        "national_real_estate_data_CLEANED['townID'] = city_ids\n",
        "# national_real_estate_data_CLEANED['bathrooms'] = bathrooms_\n",
        "\n",
        "\n",
        "\n",
        "new_tp = []\n",
        "for tp in list(national_real_estate_data_CLEANED['transactiontype']):\n",
        "  if tp == \"vanzari\":\n",
        "    new_tp.append(\"vanzare\")\n",
        "\n",
        "  elif tp == \"inchiriere\":\n",
        "    new_tp.append(\"inchiriere\")\n",
        "\n",
        "  elif tp == \"vanzare\":\n",
        "    new_tp.append(\"vanzare\")\n",
        "  \n",
        "\n",
        "\n",
        "national_real_estate_data_CLEANED['transactiontype'] = new_tp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALe7WS8um5wA"
      },
      "source": [
        "graphics class service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhsAfD4lk22y"
      },
      "outputs": [],
      "source": [
        "class Graphics():\n",
        "  def __init__(self,national_real_estate_data,national_real_estate_data_CLEANED):\n",
        "    self.national_real_estate_data = national_real_estate_data\n",
        "    self.national_real_estate_data_CLEANED = national_real_estate_data_CLEANED\n",
        "\n",
        "  def page_for_each_collector(self):\n",
        "   \n",
        "    ds = {}\n",
        "    # create data\n",
        "    colectors = list(set(self.national_real_estate_data['colector'].values))\n",
        "    for colector in colectors:\n",
        "      page = national_real_estate_data.loc[national_real_estate_data['colector'] == colector, 'pagenumber']\n",
        "      max_page = max(list(page.values))\n",
        "      ds[colector] = max_page\n",
        "     \n",
        "    keys = ds.keys()\n",
        "    values = ds.values()\n",
        "    x = np.arange(len(keys)) \n",
        "    width = 0.4\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_ylabel('')\n",
        "    ax.set_title('')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(keys)\n",
        "    pps = ax.bar(x - width / 2, values, width, label='pret')\n",
        "    for p in pps:\n",
        "      height = p.get_height()\n",
        "      ax.text(x=p.get_x() + p.get_width() / 2, y=height+.10,s=\"{}\".format(height),ha='center')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def missing_values(self):\n",
        "    msno.heatmap(self.national_real_estate_data_CLEANED)\n",
        "\n",
        "  def class_distribution(self):\n",
        "    class_counts = ds_labeled.groupby('label').size()\n",
        "    perc1 = str(round((int(class_counts['luxury_asset']) / len(ds_labeled)) * 100,2)) + '%'\n",
        "    perc2 = str(round((int(class_counts['modest_asset']) / len(ds_labeled)) * 100,2)) + '%'\n",
        "    perc1, perc2\n",
        "    class_distributions = pd.DataFrame(data=[[perc1,perc2]],columns=['luxury asset distribution','modest asset distribution'])\n",
        "\n",
        "    sns.histplot(data=self.national_real_estate_data,x='label')\n",
        "    plt.show()\n",
        "\n",
        "  def corr_matrix(self):\n",
        "    corrMatrix = self.national_real_estate_data_CLEANED[['townID','rooms','yearconstruction','confort','furnished','squaremetres','compartimentation']].corr()\n",
        "    sns.heatmap(corrMatrix, annot=True)\n",
        "    plt.show()\n",
        "\n",
        "  # de adaugat : violin plot, boxplot, pairgrid, jointplot, swarmplot\n",
        "  def joint_plot(self):\n",
        "    sns.jointplot(data=self.national_real_estate_data_CLEANED, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
        "\n",
        "  def regression_plot(self):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    sns.regplot(x=self.national_real_estate_data_CLEANED[\"price\"] , y = self.national_real_estate_data_CLEANED[\"rooms\"])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def swarmplot(self):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    sns.swarmplot(self.national_real_estate_data_CLEANED['price'], self.national_real_estate_data_CLEANED['yearconstruction'])\n",
        "    plt.show()\n",
        "\n",
        "  def violinplot(self):\n",
        "    sns.violinplot(x=\"price\", y=\"rooms\",\n",
        "                    data=self.national_real_estate_data_CLEANED)\n",
        "    \n",
        "  def pairgrid(self):\n",
        "    g = sns.PairGrid(self.national_real_estate_data_CLEANED, hue=\"townID\")\n",
        "    g.map_upper(sns.scatterplot)\n",
        "    g.map_lower(sns.kdeplot)\n",
        "    g.map_diag(sns.kdeplot)\n",
        "\n",
        "  def pieplot(self):\n",
        "    size_of_groups=[]\n",
        "    # create data\n",
        "    colectors = set(list(self.national_real_estate_data['colector'].values))\n",
        "    for colector in colectors:\n",
        "      size_of_groups.append(len(self.national_real_estate_data[self.national_real_estate_data['colector'] == colector]))\n",
        "    explode = [0.2] * len(size_of_groups)\n",
        "    plt.pie(size_of_groups, labels = colectors,  autopct='%.1f%%',  explode=explode, shadow=True)\n",
        "    plt.show()\n",
        "\n",
        "  def histograms(self):\n",
        "    fig, axs = plt.subplots(3, 2, figsize=(7, 7))\n",
        "    sns.histplot(data=self.national_real_estate_data,x='colector',color=\"skyblue\", ax=axs[0, 0])\n",
        "    sns.histplot(data=self.national_real_estate_data,x='transactiontype', color=\"olive\", ax=axs[0, 1])\n",
        "\n",
        "    sns.histplot(data=self.national_real_estate_data,y='town', color=\"gold\", ax=axs[1, 0])\n",
        "    sns.histplot(data=self.national_real_estate_data_CLEANED,x='rooms',  color=\"black\", ax=axs[1, 1])\n",
        "\n",
        "    sns.histplot(data=self.national_real_estate_data_CLEANED,x='confort', color=\"green\", ax=axs[2, 0])#.set_title('0 reprezinta cel mai inalt grad confort')\n",
        "    sns.histplot(data=self.national_real_estate_data_CLEANED,x='balcony', color=\"red\", ax=axs[2, 1])#.set_title('0 reprezinta ca nu a fost specificat, 1 exista, 2 nu exista')\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  \n",
        "  def medium_price(self, interest_area_list, title, ytitle, opdecider):\n",
        "    mean_prices_per_area = {}\n",
        "    if opdecider == 0:\n",
        "      for area_ in interest_area_list:\n",
        "        price_specific_area = self.national_real_estate_data_CLEANED.query(\"'{}' in area\".format(area_))\n",
        "        list_prices = list(price_specific_area['price'].values)\n",
        "        if len(list_prices) > 0 :\n",
        "          mean_prices_per_area[area_] = int(mean(list_prices))\n",
        "\n",
        "    elif opdecider == 1:\n",
        "      for area_ in interest_area_list:\n",
        "        price_specific_area = self.national_real_estate_data_CLEANED.query(\"yearconstruction == {}\".format(area_))\n",
        "        list_prices = list(price_specific_area['price'].values)\n",
        "        mean_prices_per_area[area_] = int(mean(list_prices))\n",
        "    \n",
        "    elif opdecider == 2:\n",
        "      for area_ in interest_area_list:\n",
        "        price_specific_area = self.national_real_estate_data_CLEANED.query(\"compartimentation == {}\".format(area_))\n",
        "        list_prices = list(price_specific_area['price'].values)\n",
        "        mean_prices_per_area[area_] = int(mean(list_prices))\n",
        "\n",
        "    mean_prices_per_area_sorted = dict(sorted(mean_prices_per_area.items(), key=lambda x: x[1], reverse=True))\n",
        "    df_prices = pd.DataFrame.from_dict([mean_prices_per_area_sorted])\n",
        "    plt.rcParams[\"figure.figsize\"] = [8, 8]\n",
        "    plt.rcParams[\"figudre.autolayout\"] = True\n",
        "    keys = mean_prices_per_area_sorted.keys()\n",
        "    values = mean_prices_per_area_sorted.values()\n",
        "    x = np.arange(len(keys)) \n",
        "    width = 0.4\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_ylabel(ytitle)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(keys)\n",
        "    pps = ax.bar(x - width / 2, values, width, label='pret')\n",
        "    for p in pps:\n",
        "      height = p.get_height()\n",
        "      ax.text(x=p.get_x() + p.get_width() / 2, y=height+.10,s=\"{}\".format(height),ha='center')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "gph = Graphics(national_real_estate_data, national_real_estate_data_CLEANED)\n",
        "gph.violinplot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classification problem - asset classification"
      ],
      "metadata": {
        "id": "Y6gxp85HC66T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "class ClassificationProblem():\n",
        "   def __init__(self, X_train,X_test, y_train, y_test,n_class):\n",
        "      self.X_train = X_train\n",
        "      self.X_test = X_test\n",
        "      self.y_train = y_train\n",
        "      self.y_test = y_test\n",
        "      self.n_class = n_class\n",
        "      \n",
        "   def knn_classifier(self, n_neighbors):\n",
        "      model = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
        "      model.fit(self.X_train, self.y_train)\n",
        "      y_pred_test = model.predict(self.X_test)\n",
        "      rep  = classification_report(self.y_test, y_pred_test)\n",
        "      print(rep)\n",
        "\n",
        "   def mlp(self):\n",
        "        mlp = MLPClassifier(\n",
        "            hidden_layer_sizes=(50, 150, 200),\n",
        "            activation='relu',\n",
        "            solver='adam',\n",
        "            learning_rate='constant',\n",
        "            early_stopping=True,\n",
        "            validation_fraction=0.2\n",
        "        )\n",
        "\n",
        "        mlp.fit(self.X_train, self.y_train)\n",
        "        predicted_test = mlp.predict(self.X_test)\n",
        "        predicted_train = mlp.predict(self.X_train)\n",
        "        rep  = classification_report(self.y_test, predicted_test)\n",
        "        print(rep)\n",
        "\n",
        "      \n",
        "   def meth_randomforest(self):\n",
        "        rf_reg = RandomForestClassifier(n_estimators = 750)\n",
        "        rf_reg.fit(self.X_train, self.y_train)\n",
        "        test_pred = rf_reg.predict(self.X_test)\n",
        "        train_pred = rf_reg.predict(self.X_train)\n",
        "        clf_rep  = classification_report(self.y_test, test_pred)\n",
        "        print(clf_rep)\n",
        "\n",
        "\n",
        "   \n",
        "   def ann(self, bsize, epochs_nr):\n",
        "        X_train = np.array(self.X_train)\n",
        "        X_test = np.array(self.X_test)\n",
        "        y_train = np.array(self.y_train)\n",
        "        y_test = np.array(self.y_test)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(32, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
        "\n",
        "        model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(self.n_class, activation='softmax'))\n",
        "\n",
        "        opt = Adam(learning_rate=1e-4)\n",
        "\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            validation_data=(X_test, y_test),\n",
        "                            batch_size=bsize,\n",
        "                            epochs=epochs_nr)\n",
        "        # validation_split=0.2)\n",
        "\n",
        "        print(model.summary())\n",
        "\n",
        "        # test_pred = model.predict(self.X_test)\n",
        "        # train_pred = model.predict(self.X_train)\n",
        "\n",
        "        try:\n",
        "            plt.plot(history.history['loss'], label='loss')\n",
        "            plt.plot(history.history['val_loss'], label='val_loss')\n",
        "\n",
        "            plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "            plt.plot(history.history['accuracy'], label='accuracy')\n",
        "\n",
        "            plt.legend(['loss', 'val_loss', 'val acc', 'acc'])\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as err:\n",
        "            print(err)\n",
        "\n",
        "   \n",
        "\n"
      ],
      "metadata": {
        "id": "T4ii-t8VDDMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_labeled['balcony'] = ds_labeled['balcony'].fillna(method='ffill').fillna(method='bfill')\n",
        "ds_labeled['parkingplaces'] = ds_labeled['parkingplaces'].fillna(method='ffill').fillna(method='bfill')\n",
        "ds_labeled['bathrooms'] = ds_labeled['bathrooms'].fillna(method='ffill').fillna(method='bfill')\n",
        "ds_labeled['townID'] = city_ids\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(ds_labeled['label'])\n",
        "\n",
        "x = ds_labeled[['townID','rooms','yearconstruction','confort','furnished','squaremetres','compartimentation','balcony','parkingplaces','bathrooms']]\n",
        "\n",
        "n_class = len(y)\n",
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.3)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_standard = scaler.fit_transform(x_train)\n",
        "x_test_standard = scaler.transform(x_test)\n",
        " \n",
        "mlCommander = ClassificationProblem(x_train_standard, x_test_standard, y_train, y_test, n_class)\n",
        "# mlCommander.ann(128,30)\n",
        "# mlCommander.mlp()\n",
        "print(\"#\"*30)\n",
        "# mlCommander.knn_classifier(4)\n",
        "print(\"#\"*30)\n",
        "mlCommander.meth_randomforest()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8mG55_H9EdS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI-APMvqPQP_"
      },
      "source": [
        "regression problem - price prediction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4ZCmy7BH7nGq"
      },
      "outputs": [],
      "source": [
        "x = national_real_estate_data_CLEANED[['townID','rooms','yearconstruction','confort','furnished','squaremetres','compartimentation','balcony','parkingplaces','bathrooms']]\n",
        "\n",
        "def cross_val(model):\n",
        "    pred = cross_val_score(model, x, y, cv=10)\n",
        "    return pred.mean()\n",
        "\n",
        "def print_evaluate(true, predicted):  \n",
        "    mae = metrics.mean_absolute_error(true, predicted)\n",
        "    mse = metrics.mean_squared_error(true, predicted)\n",
        "    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n",
        "    r2_square = metrics.r2_score(true, predicted)\n",
        "    print('MAE:', mae)\n",
        "    print('MSE:', mse)\n",
        "    print('RMSE:', rmse)\n",
        "    print('R2 Square', r2_square)\n",
        "    print('__________________________________')\n",
        "    \n",
        "def evaluate(true, predicted):\n",
        "    mae = metrics.mean_absolute_error(true, predicted)\n",
        "    mse = metrics.mean_squared_error(true, predicted)\n",
        "    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n",
        "    r2_square = metrics.r2_score(true, predicted) * 100\n",
        "    return mae, mse, rmse, r2_square\n",
        "\n",
        "class RegressionProblem():\n",
        "\n",
        "  def __init__(self, X_train,X_test, y_train, y_test):\n",
        "    self.X_train = X_train\n",
        "    self.X_test = X_test\n",
        "    self.y_train = y_train\n",
        "    self.y_test = y_test\n",
        "\n",
        "  results_df = pd.DataFrame(columns = ['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"] )\n",
        "\n",
        "  def meth_linear_regr(self):\n",
        "    lin_reg = LinearRegression(normalize=True)\n",
        "    lin_reg.fit(self.X_train,y_train)\n",
        "    coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\n",
        "    test_pred = lin_reg.predict(self.X_test)\n",
        "    train_pred = lin_reg.predict(self.X_train)\n",
        "    # print('Test set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_test, test_pred)\n",
        "    # print('Train set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_train, train_pred)\n",
        "\n",
        "    results_df_2 = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n",
        "                              columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
        "    self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
        "    \n",
        "\n",
        "  def meth_ransac(self):\n",
        "    model = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\n",
        "    model.fit(self.X_train, self.y_train)\n",
        "\n",
        "    test_pred = model.predict(self.X_test)\n",
        "    train_pred = model.predict(self.X_train)\n",
        "\n",
        "    # print('Test set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_test, test_pred)\n",
        "    # print('====================================')\n",
        "    # print('Train set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_train, train_pred)\n",
        "\n",
        "    results_df_2 = pd.DataFrame(data=[[\"Robust Regression\", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], \n",
        "                                columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
        "    self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
        "\n",
        "  def meth_ridge(self):\n",
        "    model = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\n",
        "    model.fit(self.X_train, self.y_train)\n",
        "    pred = model.predict(self.X_test)\n",
        "\n",
        "    test_pred = model.predict(self.X_test)\n",
        "    train_pred = model.predict(self.X_train)\n",
        "\n",
        "    # print('Test set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_test, test_pred)\n",
        "    # print('====================================')\n",
        "    # print('Train set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_train, train_pred)\n",
        "\n",
        "    results_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n",
        "                                columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
        "    self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
        "\n",
        "  def meth_lasso(self):\n",
        "    model = Lasso(alpha=0.1, \n",
        "              precompute=True, \n",
        "#               warm_start=True, \n",
        "              positive=True, \n",
        "              selection='random',\n",
        "              random_state=42)\n",
        "    model.fit(self.X_train, self.y_train)\n",
        "\n",
        "    test_pred = model.predict(self.X_test)\n",
        "    train_pred = model.predict(self.X_train)\n",
        "\n",
        "    # print('Test set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_test, test_pred)\n",
        "    # print('====================================')\n",
        "    # print('Train set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_train, train_pred)\n",
        "\n",
        "    results_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \n",
        "                                columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
        "    self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
        "\n",
        "  def meth_elasticnet(self):\n",
        "    model = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\n",
        "    model.fit(self.X_train, self.y_train)\n",
        "\n",
        "    test_pred = model.predict(self.X_test)\n",
        "    train_pred = model.predict(self.X_train)\n",
        "\n",
        "    # print('Test set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_test, test_pred)\n",
        "    # print('====================================')\n",
        "    # print('Train set evaluation:\\n_____________________________________')\n",
        "    # print_evaluate(y_train, train_pred)\n",
        "\n",
        "    results_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], \n",
        "                                columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
        "    self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
        "\n",
        "  def meth_polynomial(self):\n",
        "    poly_reg = PolynomialFeatures(degree=2)\n",
        "\n",
        "    X_train_2_d = poly_reg.fit_transform(self.X_train)\n",
        "    X_test_2_d = poly_reg.transform(self.X_test)\n",
        "\n",
        "    lin_reg = LinearRegression(normalize=True)\n",
        "    lin_reg.fit(X_train_2_d,self.y_train)\n",
        "\n",
        "    test_pred = lin_reg.predict(X_test_2_d)\n",
        "    train_pred = lin_reg.predict(X_train_2_d)\n",
        "\n",
        "\n",
        "    results_df_2 = pd.DataFrame(data=[[\"Polynomail Regression\", *evaluate(y_test, test_pred), 0]], \n",
        "                                columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
        "    self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
        "  \n",
        "  def ann(self, epochs_nr,bsize):\n",
        "      X_train = np.array(self.X_train)\n",
        "      X_test = np.array(self.X_test)\n",
        "      y_train = np.array(self.y_train)\n",
        "      y_test = np.array(self.y_test)\n",
        "\n",
        "      model = Sequential()\n",
        "\n",
        "      model.add(Dense(X_train.shape[1], activation='relu'))\n",
        "      model.add(Dense(64, activation='relu'))\n",
        "      model.add(Dropout(0.3))\n",
        "\n",
        "      model.add(Dense(128, activation='relu'))\n",
        "      model.add(Dropout(0.3))\n",
        "\n",
        "      model.add(Dense(256, activation='relu'))\n",
        "      model.add(Dropout(0.4))\n",
        "\n",
        "      model.add(Dense(512, activation='relu'))\n",
        "      model.add(Dropout(0.5))\n",
        "\n",
        "      model.add(Dense(1024, activation='relu'))\n",
        "      model.add(Dropout(0.5))\n",
        "\n",
        "      # model.add(Dense(2048, activation='relu'))\n",
        "      # model.add(Dropout(0.5))\n",
        "\n",
        "      model.add(Dense(512, activation='relu'))\n",
        "      model.add(Dropout(0.5))\n",
        "\n",
        "      model.add(Dense(256, activation='relu'))\n",
        "      model.add(Dropout(0.5))\n",
        "\n",
        "      # model.add(Dense(128, activation='relu'))\n",
        "      # model.add(Dropout(0.4))\n",
        "\n",
        "      # model.add(Dense(64, activation='relu'))\n",
        "      # model.add(Dropout(0.2))\n",
        "\n",
        "      model.add(Dense(1))\n",
        "\n",
        "      model.compile(optimizer='adam', loss='huber')\n",
        "\n",
        "\n",
        "      history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test,y_test),\n",
        "                    batch_size=bsize,\n",
        "                    epochs=epochs_nr)\n",
        "                    #validation_split=0.2)\n",
        "      \n",
        "      print(model.summary())\n",
        "\n",
        "      test_pred = model.predict(X_test)\n",
        "      train_pred = model.predict(X_train)\n",
        "\n",
        "      # print('Test set evaluation:\\n_____________________________________')\n",
        "      # print_evaluate(y_test, test_pred)\n",
        "\n",
        "      # print('Train set evaluation:\\n_____________________________________')\n",
        "      # print_evaluate(y_train, train_pred)\n",
        "\n",
        "      results_df_2 = pd.DataFrame(data=[[\"Artficial Neural Network\", *evaluate(y_train, train_pred), 0]], \n",
        "                                  columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
        "      self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
        "\n",
        "\n",
        "      try:\n",
        "          plt.plot(history.history['val_loss'], label='val loss')\n",
        "          plt.plot(history.history['loss'], label='loss')\n",
        "\n",
        "          plt.legend(['val loss','loss'])\n",
        "          plt.show()\n",
        "      \n",
        "      except Exception as err:\n",
        "          print(err)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  def meth_randomforest(self):\n",
        "    # from sklearn.model_selection import GridSearchCV\n",
        "    # param_grid = {\n",
        "    # 'bootstrap': [True],\n",
        "    # 'max_depth': [80, 90, 100, 110],\n",
        "    # 'max_features': [2, 3],\n",
        "    # 'min_samples_leaf': [3, 4, 5],\n",
        "    # 'min_samples_split': [8, 10, 12],\n",
        "    # 'n_estimators': [100, 200, 300, 1000]\n",
        "    # }\n",
        "    # # Create a based model\n",
        "    # rf = RandomForestRegressor()\n",
        "    # # Instantiate the grid search model\n",
        "    # grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "    #                           cv = 2, n_jobs = -1, verbose = 2)\n",
        "    # grid_search.fit(self.X_train, self.y_train)\n",
        "    # print(grid_search.best_params_)\n",
        "\n",
        "    rf_reg = RandomForestRegressor(bootstrap = True, max_depth = 110, max_features = 3, min_samples_leaf = 3, min_samples_split =  8, n_estimators = 1500)\n",
        "    rf_reg.fit(self.X_train, self.y_train)\n",
        "\n",
        "    test_pred = rf_reg.predict(self.X_test)\n",
        "    train_pred = rf_reg.predict(self.X_train)\n",
        "\n",
        "\n",
        "    results_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, test_pred), 0]], \n",
        "                                columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
        "    self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1loTbmHGra9"
      },
      "source": [
        "running regression problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451BhmHQ9GOi"
      },
      "outputs": [],
      "source": [
        "filling_method = \"bfill\"\n",
        "\n",
        "ds_labeled['balcony'] = ds_labeled['balcony'].fillna(method=filling_method)\n",
        "ds_labeled['parkingplaces'] = ds_labeled['parkingplaces'].fillna(method=filling_method)\n",
        "ds_labeled['bathrooms'] = ds_labeled['bathrooms'].fillna(method=filling_method)\n",
        "ds_labeled['townID'] = city_ids\n",
        "y = ds_labeled['price']\n",
        "\n",
        "x = ds_labeled[['townID','rooms','yearconstruction','confort','furnished','squaremetres','compartimentation','balcony','parkingplaces','bathrooms']]\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.33)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_standard = scaler.fit_transform(x_train)\n",
        "x_test_standard = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "mlCommander = RegressionProblem(x_train_standard, x_test_standard, y_train, y_test)\n",
        "\n",
        "# mlCommander.meth_elasticnet()\n",
        "print(\"working with \".format(filling_method))\n",
        "mlCommander.ann(100, 128)\n",
        "# mlCommander.meth_polynomial()\n",
        "\n",
        "# mlCommander.meth_randomforest()\n",
        "# mlCommander.meth_linear_regr()\n",
        "# mlCommander.meth_ransac()\n",
        "# mlCommander.meth_ridge()\n",
        "\n",
        "model_scores = mlCommander.results_df\n",
        "print(model_scores.to_string())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "info_visualization_project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIGRrtnL4RqBGKKrvdkEjf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}